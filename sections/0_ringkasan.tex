Apache Hadoop adalah kerangka kerja sumber terbuka yang digunakan untuk pemrosesan terdistribusi dan analisis himpunan data besar pada kluster. Hadoop menyediakan kemampuan penyimpanan data besar dan menjalankan aplikasi pada kluster perangkat keras komoditas. Kerangka kerja ini juga mencakup modul Apache Hadoop MapReduce, yang memungkinkan penulisan pekerjaan untuk memproses sejumlah besar data dengan membagi data input menjadi gugus-gugus independen yang diproses secara paralel di seluruh simpul dalam kluster. Selain itu, ekosistem Hadoop mencakup perangkat lunak dan utilitas terkait seperti Apache Hive, Apache HBase, Spark, Kafka, dan banyak lagi. Dengan kemampuannya untuk menyimpan dan memproses dataset besar secara efisien, Hadoop menjadi penting karena dapat mengatasi peningkatan volume dan variasi data, terutama dari media sosial dan internet.